{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from typing import Type, Optional, Union, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet: Overall Architecture and Specifications\n",
    "\n",
    "<img src=\"images/Resnet_architecture.png\" width=420, height=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes while implementing Resnet\n",
    "- In the first block of each layer (except for the first block), the first convolutional layer is with stride 2 -> decrease the feature map, increase the number of channels\n",
    "   - Why? Because after each layer, the size of feature map is reduced by 2\n",
    "- Each layer has one block in which we have to downsample the input (make the input have the same channels at the output to concatenate), which is the first block in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![Resnet-layers](images/Resnet.png) -->\n",
    "Specification of the number of blocks and layers from each block of different variations of Resnet\n",
    "<img src=\"images/Resnet_layers_specs.png\" width=800, height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block, a basic Unit in Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first block we're going to implement is a **BasicBlock**, which consist of only 2 convolutional layers\n",
    "- This type of block is used in Resnet18 and Resnet34\n",
    "- The number of channels in the block's input and output doesn't change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock (nn.Module) :\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 downsample: Optional[nn.Sequential] = None,\n",
    "                 stride: int = 1) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        # self.expansion = 4 -> there's no expansion of layers in BasicBlock\n",
    "\n",
    "        self.conv1 = nn.Conv2d (in_channels=self.in_channels, \n",
    "                                out_channels=self.out_channels, \n",
    "                                kernel_size=3, stride=self.stride)  # stride = 2 only first convolutional layer of first block, except first block\n",
    "        self.batch_norm1 = nn.BatchNorm2d(self.out_channels)\n",
    "        self.conv2 = nn.Conv2d (in_channels=self.out_channels,\n",
    "                                out_channels=self.out_channels,\n",
    "                                kernel_size=3)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)       # inplace: directly modify the input without allocating memory for the output\n",
    "        \n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward (self, X: Tensor) -> Tensor:\n",
    "        identity = X\n",
    "\n",
    "        feat_map1 = self.conv1(X)\n",
    "        norm1 = self.batch_norm1(feat_map1)\n",
    "        activation1 = self.relu(norm1)\n",
    "\n",
    "        feat_map2 = self.conv2(activation1)\n",
    "        norm2 = self.batch_norm2(feat_map2)\n",
    "        # activation2 = self.relu(norm2)        # the last output doesn't need ReLU\n",
    "        output = norm2                          # redundant, included for clear interpretation\n",
    "\n",
    "        if self.downsample != None:\n",
    "            identity = self.downsample(identity)\n",
    "        output += identity\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second kind of block is **BottleNeck**, which includes three convolutional layers, two with 1x1 kernels, and one with a 3x3 kernel\n",
    "- The first convolutional layer (with the 1x1 kernel) has stride=2\n",
    "- The number of channels of the input and output of the block is different in the first block of each \"layer\" (output channels = input channels x 4); in the successive blocks, this number is similar in both the input and output\n",
    "    - So to concate the identity matrix (input of each block) to the output, we have to first check if they are already at the same size and have the same channels\n",
    "    - If that is not the case, we have to **downsample** the input (the identity matrix), the downsample function is passed from the Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck (nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int,\n",
    "                 downsample: Optional[nn.Sequential] = None,\n",
    "                 stride: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.expansion = 4\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.in_channels,\n",
    "                               out_channels=self.out_channels,\n",
    "                               kernel_size=1, stride=stride)        # stride of the first convolutional layer can be either 1 or 2\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.out_channels,\n",
    "                               out_channels=self.out_channels,\n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "        self.conv3 = nn.Conv2d(in_channels=self.out_channels,\n",
    "                               out_channels=self.out_channels*self.expansion,\n",
    "                               kernel_size=1)                       # default stride is 1\n",
    "        self.batch_norm3 = nn.BatchNorm2d(num_features=self.out_channels*self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        identity = X\n",
    "\n",
    "        feat_map1 = self.conv1(X)\n",
    "        norm1 = self.batch_norm1(feat_map1)\n",
    "        activation1 = self.relu(norm1)\n",
    "\n",
    "        feat_map2 = self.conv2(activation1)\n",
    "        norm2 = self.batch_norm2(feat_map2)\n",
    "        activation2 = self.relu(norm2)\n",
    "\n",
    "        feat_map3 = self.conv3(activation2)\n",
    "        norm3 = self.batch_norm3(feat_map3)\n",
    "        output = norm3\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.identity()\n",
    "        output += identity\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1491111416.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    block: Type[Union[BasicBlock, BottleNeck]]\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "class Resnet (nn.Module):\n",
    "    def __init__(self, \n",
    "                 block_t: Type[Union[BasicBlock, BottleNeck]], \n",
    "                 n_blocks: List[int],\n",
    "                 n_classes: int = 1000) -> None:\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.block = block_t\n",
    "        self.expansion = self.block.expansion\n",
    "        self.in_channels = 64          # every variant of Resnet has an input size of 64 for the first layer\n",
    "\n",
    "        self.layer1 = self._make_layer (block=self.block, n_blocks=self.n_blocks[0], \n",
    "                                        in_channels=self.in_channels, out_channels=64, \n",
    "                                        stride=1)    # the first layer have stride 1 -> no cchange in the output feature map's size\n",
    "        self.layer2 = self._make_layer (block=self.block, n_blocks=self.n_blocks[1], \n",
    "                                        in_channels=self.in_channels, out_channels=128,\n",
    "                                        stride=2)    # following layers have stride 2 (feature map's size reduce after each layer)\n",
    "        self.layer3 = self._make_layer (block=self.block, n_blocks=self.n_blocks[2], \n",
    "                                        in_channels=self.in_channels, out_channels=256,\n",
    "                                        stride=2)\n",
    "        self.layer4 = self._make_layer (block=self.block, n_blocks=self.n_blocks[3], \n",
    "                                        in_channels=self.in_channels, out_channels=512,\n",
    "                                        stride=2)\n",
    "\n",
    "    def _make_layer (self, \n",
    "                     block: Type[Union[BasicBlock, BottleNeck]],\n",
    "                     n_blocks: int,\n",
    "                     in_channels: int, \n",
    "                     out_channels: int, \n",
    "                     stride: int):\n",
    "        layers = []\n",
    "        # if stride = 1 -> there's no decrease in feature map -> no need for downsampling\n",
    "        # if in_channels = out_channels*4 -> we are not in the first block -> no need for downsampling, only do downsampling for the first block\n",
    "        # -> no need for downsampling\n",
    "        if stride != 1 or in_channels != out_channels*self.expansion:   \n",
    "            downsample = nn.Sequential(nn.Conv2d (in_channels=in_channels,             # reduce the feature map by stride (2) -> no padding, stride=stride\n",
    "                                                  out_channels=out_channels*self.expansion,\n",
    "                                                  kernel_size=1, stride=stride, padding=0))\n",
    "        layers.append(block(in_channels=in_channels, out_channels=out_channels, downsample=downsample))\n",
    "        for _ in range (1, n_blocks):\n",
    "            layers.append(block(in_channels=in_channels, out_channels=out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward ():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional notes:\n",
    "- Conv1x1 is usually used for changing the number of channels\n",
    "- A ReLU layer with the same input size can be used for multiple layers since it doesn't have learnable parameters\n",
    "- Default value of: padding=0, stride=1, dilation=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
